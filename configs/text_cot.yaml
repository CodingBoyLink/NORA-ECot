# Text CoT LoRA Training Configuration (Phase B)
# Text ECoT + Reasoning Dropout (Full CoT / No CoT modes)
# Requirements: 9.1, 9.2, 9.3, 9.4, 9.5, 9.6

# Model configuration
base_model: "declare-lab/nora"
fast_tokenizer: "physical-intelligence/fast"

# Training phase
phase: "text_cot"

# LIBERO subset (override per experiment)
libero_subset: "object"  # spatial | object | goal | long

# LoRA configuration (Req 9.1, 9.2, 9.3)
lora:
  rank: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"

# Optimizer configuration (Req 9.4, 9.8)
optimizer:
  learning_rate: 5.0e-5
  betas: [0.9, 0.95]
  weight_decay: 1.0e-8
  warmup_steps: 1000

# Reasoning Dropout configuration (Req 9.5, 9.6)
reasoning_dropout:
  prob: 0.5  # p_CoT: probability of using Full CoT mode
  cot_loss_weight: 0.5  # lambda: weight for CoT auxiliary loss

# Training hyperparameters
per_device_batch_size: 16
gradient_accumulation_steps: 2
max_train_steps: 100000

# Mixed precision (Req 9.7)
mixed_precision: "bf16"

# Data configuration
data_dir: "./data"
train_split_ratio: 0.95
data_seed: 42

# Output configuration
output_dir: "./outputs"
checkpoint_save_frequency: 20000
logging_steps: 100

# Evaluation configuration
eval_steps: 5000
num_eval_trials: 50
max_eval_steps: 500
num_steps_wait: 10

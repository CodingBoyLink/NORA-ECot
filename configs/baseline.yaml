# Baseline LoRA Training Configuration (Phase A)
# Pure behavior cloning - only predicts actions
# Requirements: 9.1, 9.2, 9.3, 9.4

# Model configuration
base_model: "declare-lab/nora"
fast_tokenizer: "physical-intelligence/fast"

# Training phase
phase: "baseline"

# LIBERO subset (override per experiment)
libero_subset: "spatial"  # spatial | object | goal | 10

# LoRA configuration (Req 9.1, 9.2, 9.3)
lora:
  rank: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"

# Optimizer configuration (Req 9.4, 9.8)
optimizer:
  learning_rate: 5.0e-5
  betas: [0.9, 0.95]
  weight_decay: 1.0e-8
  warmup_steps: 1000

# Reasoning Dropout (not used in baseline, but included for consistency)
reasoning_dropout:
  prob: 0.0
  cot_loss_weight: 0.0

# Training hyperparameters
# 4090 24GB 配置：batch_size=8, grad_accum=4
# 有效batch size = 8 * 4 * 2卡 = 64
# ~1,467 steps/epoch, 10k steps ≈ 7 epochs
per_device_batch_size: 8
gradient_accumulation_steps: 4
max_train_steps: 10000

# Mixed precision (Req 9.7)
mixed_precision: "bf16"

# Data configuration
data_dir: "./data"
train_split_ratio: 0.95
data_seed: 42
num_workers: 4  # DataLoader workers (8核CPU建议4)

# Output configuration
output_dir: "./outputs"
checkpoint_save_frequency: 2000
logging_steps: 50

# Evaluation configuration (预留，当前训练代码未使用)
# 评估需要单独运行 scripts/evaluate.py
eval_steps: 5000
num_eval_trials: 50
max_eval_steps: 500
num_steps_wait: 10

# Baseline LoRA Training - LIBERO-10
# Inherits from baseline.yaml with subset-specific settings

# Training phase
phase: "baseline"

# LIBERO subset
libero_subset: "10"

# LoRA configuration
lora:
  rank: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"

# Optimizer configuration
optimizer:
  learning_rate: 5.0e-5
  betas: [0.9, 0.95]
  weight_decay: 1.0e-8
  warmup_steps: 1000

# Output configuration
output_dir: "./outputs/baseline/10"
